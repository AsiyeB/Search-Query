{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Jz-qPEoCXwdEuOLAyY49Bsgwz7J4CAEm","authorship_tag":"ABX9TyPT1zdVLjW/p5KlZKec+IYZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Download libraries and dependencies"],"metadata":{"id":"rhf26wQ07KGo"}},{"cell_type":"code","source":["!pip install openai\n","!pip install tiktoken\n","!pip install PyPDF2\n","!pip install wordninja\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2uniTlMt7RnX","executionInfo":{"status":"ok","timestamp":1687947301360,"user_tz":-210,"elapsed":172791,"user":{"displayName":"Asiye Bahrami","userId":"02646074617922160720"}},"outputId":"5d846487-659b-48a8-d2fc-61e8115ddc93"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai\n","  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n","Installing collected packages: openai\n","Successfully installed openai-0.27.8\n","Collecting tiktoken\n","  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n","Installing collected packages: tiktoken\n","Successfully installed tiktoken-0.4.0\n","Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n","Collecting wordninja\n","  Downloading wordninja-2.0.0.tar.gz (541 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.6/541.6 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: wordninja\n","  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wordninja: filename=wordninja-2.0.0-py3-none-any.whl size=541531 sha256=71b0b2e2337229f7101228866028852fd8aec38abb7f61e30584715f0d998450\n","  Stored in directory: /root/.cache/pip/wheels/aa/44/3a/f2a5c1859b8b541ded969b4cd12d0a58897f12408f4f51e084\n","Successfully built wordninja\n","Installing collected packages: wordninja\n","Successfully installed wordninja-2.0.0\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["Import libraries"],"metadata":{"id":"EN2Dc-6y8DfC"}},{"cell_type":"code","source":["import os\n","import re\n","import openai\n","import numpy as np\n","from PyPDF2 import PdfReader\n","from nltk.tokenize import sent_tokenize\n","from openai.embeddings_utils import get_embedding, cosine_similarity, get_embeddings\n","from nltk.corpus import stopwords\n","import glob\n","from timeit import default_timer as timer\n","import wordninja"],"metadata":{"id":"tvW1Zps-8Gk8","executionInfo":{"status":"ok","timestamp":1687947308231,"user_tz":-210,"elapsed":1846,"user":{"displayName":"Asiye Bahrami","userId":"02646074617922160720"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Reading PDF files, Extracting text, and cleaning"],"metadata":{"id":"rYlWyyHG8R4u"}},{"cell_type":"code","source":["def remove_empty_strings(text, sentences):\n","  for index, sent in enumerate(sentences):\n","    if len(sent.split(' '))==1:\n","      text.pop(index)\n","      sentences.pop(index)\n","  return sentences, text\n","\n","def clean_text(text):\n","\n","  text = text[:text.find('References')]\n","  text = re.sub(r'([a-z])\\.([A-Z])', r'\\1. \\2', text)\n","  text = text.lower()\n","  regex_dict = {\n","    'reference_chapter_authors': (r'\\[\\d+(?:,\\s*\\d+)*\\]'\"|\"r'\\d+(?:\\.\\d+)*\\.'\"|\"r'\\b\\w+\\s+et al\\.|\\b\\w+,?etal\\.', \"\"),\n","    'next_line': ('\\n', \" \"),\n","    'continued_line': ('- +', \"\"),\n","    'multiple_spaces': (' +', ' '),\n","    'pages': (r'(p|q)\\s?\\d+', ''),\n","    'figs': (r\"fig\\.(\\s)?\\d?|figs\\.(\\s)?\\d?\",'')\n","  }\n","\n","  for key, value in regex_dict.items():\n","    text = re.sub(value[0], value[1], text)\n","\n","  return text\n","\n","def extract_text_from_pdf(pdf_path):\n","    reader = PdfReader(pdf_path)\n","    text = \"\"\n","    for page in reader.pages:\n","        text += page.extract_text()\n","    text = clean_text(text)\n","    sentences = sent_tokenize(text)\n","    sentences_to_embed = []\n","\n","    splitjoined = []\n","\n","    for sent in sentences:\n","      splitjoined.append(wordninja.split(sent))\n","\n","    cleaned = [' '.join(i) for i in splitjoined]\n","\n","    whquestion = ['what', 'which', 'who', 'whom', 'there', 'when', 'where', 'why', 'how']\n","    stop_words = [i for i in stopwords.words('english') if i not in whquestion]\n","    for sent in cleaned:\n","      content = \"\"\n","      for word in sent.split(' '):\n","        if word not in stop_words:\n","          content = content + \" \" + word\n","      sentences_to_embed.append(content)\n","\n","    return remove_empty_strings(sentences_to_embed, cleaned)\n","\n","def get_files_content(path):\n","   files = glob.glob(f\"{path}/*.pdf\")\n","\n","   all_sentences = []\n","   all_sent_embeds = []\n","   for file in files:\n","    sentences, text = extract_text_from_pdf(file)\n","    all_sentences.extend(sentences)\n","    all_sent_embeds.extend(text)\n","   return all_sentences, all_sent_embeds"],"metadata":{"id":"IF0bMgqp8He5","executionInfo":{"status":"ok","timestamp":1687947319712,"user_tz":-210,"elapsed":514,"user":{"displayName":"Asiye Bahrami","userId":"02646074617922160720"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Get embeddings for sentences from OpenAI API"],"metadata":{"id":"lGwkG4GZ8f-t"}},{"cell_type":"code","source":["def search_for_query(query, text, sentences):\n","  print(\"#extracted sentences: \", len(text))\n","  model = \"text-embedding-ada-002\"\n","  query_embedding = get_embedding(query, engine=model)\n","  text_embeddings = []\n","\n","  for i in range(0, len(text), 500):\n","    text_embeddings.extend(get_embeddings(text[i:i+500],engine=model))\n","\n","  similarities = [cosine_similarity(t, query_embedding) for t in text_embeddings]\n","  indicies = (-(np.array(similarities))).argsort()[:10]\n","\n","  results = [sentences[i] for i in indicies]\n","  return results"],"metadata":{"id":"MaEiVzSI8b80","executionInfo":{"status":"ok","timestamp":1687947326823,"user_tz":-210,"elapsed":3,"user":{"displayName":"Asiye Bahrami","userId":"02646074617922160720"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Getting it, altogether"],"metadata":{"id":"gR591ni_8t4m"}},{"cell_type":"code","source":["def main():\n","  os.environ['OPENAI_API_KEY'] = 'insert your openai api key here or in a .env file'\n","  openai.api_key = os.getenv('OPENAI_API_KEY')\n","\n","  query = 'What are the effects of fake reviews?'\n","  start = timer()\n","  sents, sents_to_embed =  get_files_content('insert drive path of pdf files')\n","  print(\"reading time:\",\"{:.2f}\".format(timer()-start),\"s\")\n","  start = timer()\n","  results = search_for_query(query, sents_to_embed, sents)\n","\n","  print(\"\\nresults for query:\\n\")\n","  for result in results:\n","    print(\"•\",result)\n","  print(\"\\nembedding time:\",\"{:.2f}\".format(timer()-start),\"s\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFwfGeJV8q-G","executionInfo":{"status":"ok","timestamp":1687947392985,"user_tz":-210,"elapsed":50582,"user":{"displayName":"Asiye Bahrami","userId":"02646074617922160720"}},"outputId":"b4f258b4-ea8c-4aad-d1de-df3cafebca52"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["reading time: 38.53 s\n","#extracted sentences:  2611\n","\n","results for query:\n","\n","• in particular fake reviews undermine market efficacy and have a negative effect on social welfare\n","• to some extent the effects of fake reviews are decided by how a platform operates\n","• the final effects of fake reviews on the platforms are moderated by consumers ' attitudes toward fake reviews\n","• effects on various stakeholders fake reviews significantly affect various stakeholders such as consumers merchants and platforms\n","• on stakeholders fake reviews increase uncertainty and cause consumer distrust and the psychological discomfort whereas fake reviews directly increase purchase intentions increased distrust and psychological discomfort weaken purchase intentions and create negative word of mouth for products extant studies have not researched a consensus on the effects of fake reviews on product sales\n","• although fake reviews destroy the reputation of platforms the additional profits gained should be higher than the potential loss of the affected reputation\n","• the existing literature largely focuses on the detrimental effects of fake reviews but completely disregards their benefits\n","• if consumers do not rely on existing online products when posting online reviews and only express their genuine feelings about products then fake reviews would have minimal effects on the development of online reviews\n","• existing studies reveal that positive fake reviews are more common than negative fake reviews\n","• f can fake reviews bring some benefits\n","\n","embedding time: 11.15 s\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"IqTKL4bJ82I2"},"execution_count":null,"outputs":[]}]}